# --- IMPORTS et FONCTION DE PR√âPARATION ---
import os
import numpy as np
from datasets import load_dataset 
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from evaluate import load 

# --- CONFIGURATION GLOBALE ---
MODEL_NAME = "bert-base-multilingual-cased"
MAX_LENGTH = 128
DATASET_NAME = "go_emotions"

# --- FONCTION DE PR√âPARATION DES DONN√âES ---
def load_and_prepare_data():
    print(f"1. Chargement du jeu de donn√©es '{DATASET_NAME}'...")
    raw_datasets = load_dataset(DATASET_NAME)

    id2label = raw_datasets["train"].features["labels"].feature.names
    print(f" ¬† => {len(id2label)} cat√©gories d'√©motions brutes trouv√©es.")
    
    label2id = {name: i for i, name in enumerate(id2label)}
    
    def simplify_labels(example):
        if example["labels"]:
            # On prend uniquement la premi√®re √©tiquette pour le fine-tuning simple
            example["label"] = example["labels"][0] 
        else:
            # Assigner √† "neutral" si la liste est vide (tr√®s rare)
            example["label"] = label2id['neutral']
        return example

    print("\n2. Simplification des √©tiquettes (mono-label)...")
    simplified_datasets = raw_datasets.map(simplify_labels)

    # Suppression des colonnes qui ne sont plus n√©cessaires ou qui interf√®rent
    # On retire "author" par pr√©caution, car il n'est pas toujours l√†
    simplified_datasets = simplified_datasets.remove_columns(["labels", "id"]) 

    print("\n3. Tokenisation des donn√©es...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    
    def tokenize_function(examples):
        return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=MAX_LENGTH)

    tokenized_datasets = simplified_datasets.map(tokenize_function, batched=True)
    print(" ¬† => Tokenisation termin√©e.")

    print("\n4. Mise au format PyTorch et nettoyage final...")
    # Renommer "label" en "labels" est crucial pour le Trainer
    final_datasets = tokenized_datasets.rename_column("label", "labels")
    final_datasets.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
    
    train_dataset = final_datasets["train"]
    eval_dataset = final_datasets["test"]
    
    print(" ¬† => Pr√©paration des donn√©es termin√©e.")
    return train_dataset, eval_dataset, id2label, label2id, MODEL_NAME

# --- FONCTION DE METRIQUES ET ENTRA√éNEMENT ---

# Chargement de la m√©trique F1 (d√©pend de scikit-learn)
metric = load("f1") 

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    # Calcul du F1-score pond√©r√© pour les 28 classes
    results = metric.compute(predictions=predictions, references=labels, average="weighted")
    # Ajout de l'accuracy pour la visibilit√©
    accuracy = np.mean(predictions == labels)
    results["accuracy"] = accuracy
    return results


if __name__ == "__main__":
    
    # 1. CHARGEMENT DES DONN√âES ET CONFIGURATION
    train_dataset, eval_dataset, id2label, label2id, model_name = load_and_prepare_data()

    print(f"Chargement du mod√®le pr√©-entra√Æn√©: {model_name}...")
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name, 
        num_labels=len(id2label), 
        id2label=id2label, 
        label2id=label2id
    )
    # Le message "Some weights were not initialized..." est normal ici, car on ajoute le calque final.

   # 2. D√âFINITION DES ARGUMENTS D'ENTRA√éNEMENT (VERSION MINIMALE)
    print("\n2. D√©finition des arguments d'entra√Ænement (minimal)...")
    training_args = TrainingArguments(
        output_dir='./results',
        num_train_epochs=3,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=64,
       eval_strategy="epoch",        # ANCIENNEMENT evaluation_strategy
        save_strategy="epoch",
        load_best_model_at_end=True,
         report_to="none",
    )

    # 3. CR√âATION ET LANCEMENT DU TRAINER
    print("\n3. Cr√©ation du Trainer...")
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        compute_metrics=compute_metrics,
    )

    print("\nüöÄ D√©marrage du Fine-Tuning... Cela peut prendre beaucoup de temps sur CPU.")
    trainer.train()

    FINAL_MODEL_DIR = "./final_emotion_model"
    # Sauvegarde du mod√®le entra√Æn√© (le meilleur)
    trainer.save_model(FINAL_MODEL_DIR) 
    
    # Sauvegarde du tokenizer (tr√®s important pour l'inf√©rence)
    trainer.tokenizer.save_pretrained(FINAL_MODEL_DIR)
    
    print(f"\n‚úÖ Mod√®le entra√Æn√© et sauvegard√© sous : {FINAL_MODEL_DIR}")